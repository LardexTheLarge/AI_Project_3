{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz  \n",
    "from langchain.agents import initialize_agent, load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Store the API key in a variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"gpt2\",\n",
    "#     task=\"text-generation\",\n",
    "#     device_map='auto',\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 20}\n",
    "# )\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# model_id = \"gpt2-large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map='auto')\n",
    "\n",
    "# hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # Define the query\n",
    "# query = \"What is Godzilla's Power?\"\n",
    "\n",
    "# # Combine the format and query into a prompt\n",
    "# prompt = format_template.format(query=query)\n",
    "\n",
    "# # Calculate the maximum context length (including prompt)\n",
    "# max_context_length = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "# # Set the desired maximum number of new tokens to generate\n",
    "# max_new_tokens = 200\n",
    "\n",
    "# # Generate text using the prompt with the specified maximum number of tokens\n",
    "# output = pipe(prompt, max_length=max_context_length + max_new_tokens, min_length=max_context_length, eos_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "# # Extract the generated text and tokens\n",
    "# generated_text = output[0][\"generated_text\"]\n",
    "# generated_tokens = tokenizer(generated_text)[\"input_ids\"]\n",
    "\n",
    "# # Ensure the generated text does not exceed the max_new_tokens\n",
    "# if len(generated_tokens) > max_context_length + max_new_tokens:\n",
    "#     # Truncate the tokens to the desired length\n",
    "#     truncated_tokens = generated_tokens[max_context_length:max_context_length + max_new_tokens]\n",
    "#     # Convert the truncated tokens back to text\n",
    "#     answer = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "# else:\n",
    "#     # Extract only the answer part from the generated text\n",
    "#     answer = generated_text.split(\"-------------------------------------\")[-1].strip()\n",
    "\n",
    "# # Display the answer\n",
    "# print(answer)\n",
    "\n",
    "# def pred(info, query, tokens):\n",
    "#     # Combine the format and query into a prompt\n",
    "#     prompt = info.format(query=query)\n",
    "\n",
    "#     # Calculate the maximum context length (including prompt)\n",
    "#     max_context_length = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "#     # Set the desired maximum number of new tokens to generate\n",
    "#     max_new_tokens = int(tokens)\n",
    "\n",
    "#     # Generate text using the prompt with the specified maximum number of tokens\n",
    "#     output = pipe(prompt, max_length=max_context_length + max_new_tokens, min_length=max_context_length, eos_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "#     # Extract the generated text and tokens\n",
    "#     generated_text = output[0][\"generated_text\"]\n",
    "#     generated_tokens = tokenizer(generated_text)[\"input_ids\"]\n",
    "\n",
    "#     # Ensure the generated text does not exceed the max_new_tokens\n",
    "#     if len(generated_tokens) > max_context_length + max_new_tokens:\n",
    "#         # Truncate the tokens to the desired length\n",
    "#         truncated_tokens = generated_tokens[max_context_length:max_context_length + max_new_tokens]\n",
    "#         # Convert the truncated tokens back to text\n",
    "#         answer = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "#     else:\n",
    "#         # Extract only the answer part from the generated text\n",
    "#         answer = generated_text.split(\"\\n\\n\")[-1].strip()\n",
    "\n",
    "#     return answer\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# textGen = pipeline(\"text-generation\", model='EleutherAI/gpt-neo-1.3B', framework='pt')\n",
    "# Godzilla is a fictional giant monster originating from Japanese cinema. It first appeared\n",
    "# in the 1954 film \"Godzilla,\" directed by Ishirō Honda. Godzilla is often depicted as a\n",
    "# prehistoric sea monster awakened and empowered by nuclear radiation, symbolizing the\n",
    "# consequences of atomic weapons. The character has since become a cultural icon,\n",
    "# appearing in numerous films, television series, and other media.\n",
    "\n",
    "# prefix = \"\"\"\n",
    "# Here are examples between a human and AI. The human provides a question about Godzilla, and\n",
    "# the AI provides a single sentence with easy to read words The sentence does not have to include the \n",
    "# original question. For example:\n",
    "# \"\"\"\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"query\": \"Who is godzilla?\",\n",
    "#         \"answer\": \"Godzilla is a fictional giant monster originating from Japanese cinema. It first appeared in the 1954 film 'Godzilla,' directed by Ishirō Honda. Godzilla is often depicted as a prehistoric sea monster awakened and empowered by nuclear radiation, symbolizing the consequences of atomic weapons. The character has since become a cultural icon, appearing in numerous films, television series, and other media.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"query\": \"What is Godzilla's Power?\",\n",
    "#         \"answer\": \"Godzilla's powers vary depending on the iteration, but he is typically depicted as having immense size and strength, able to withstand powerful attacks and cause widespread destruction. In some versions, Godzilla can also emit atomic breath, a powerful beam of radiation that he uses to attack his enemies.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"query\": \"Who is Godzilla's friend?\",\n",
    "#         \"answer\": \"In various movies and series, Godzilla has been portrayed both as a destructive force and a protector of humanity. Throughout the franchise, Godzilla has formed alliances with other monsters like Mothra, Rodan, and King Kong to combat common foes. Mothra, in particular, is often depicted as a friend and ally of Godzilla, aiding him in battles against powerful adversaries.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# format = \"\"\"\n",
    "# Human: {query}\n",
    "# AI: {answer}\n",
    "# \"\"\"\n",
    "# example_template = PromptTemplate(\n",
    "#     input_variables=[\"query\", \"answer\"],\n",
    "#     template=format\n",
    "# )\n",
    "\n",
    "# suffix = \"\"\"\n",
    "# Human: {query}\n",
    "# AI: \n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = FewShotPromptTemplate(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_template,\n",
    "#     input_variables=[\"query\"],\n",
    "#     prefix=prefix,\n",
    "#     suffix=suffix,\n",
    "#     example_separator=\"\\n\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Godzilla is a fictional monster, or kaiju, that debuted in the eponymous 1954 film. The character has since become an international pop culture icon, appearing in various media such as films, television shows, video games, novels, and comic books. Godzilla is a prehistoric reptilian monster awakened and empowered by nuclear radiation, and has been featured in a total of 38 films within the Godzilla franchise.\n"
     ]
    }
   ],
   "source": [
    "# Define the format for the template.\n",
    "format_template = \"\"\"\n",
    "Godzilla is a fictional giant monster originating from Japanese cinema. It first appeared\n",
    "in the 1954 film \"Godzilla,\" directed by Ishirō Honda. Godzilla is often depicted as a\n",
    "prehistoric sea monster awakened and empowered by nuclear radiation, symbolizing the\n",
    "consequences of atomic weapons. The character has since become a cultural icon,\n",
    "appearing in numerous films, television series, and other media.\n",
    "\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# Set up the built-in wikipedia tool.\n",
    "tools = load_tools(['wikipedia'], llm=llm)\n",
    "\n",
    "# Initialize the agent.\n",
    "agent = initialize_agent(tools, agent=\"chat-zero-shot-react-description\", llm=llm)\n",
    "\n",
    "# Define the query as a string.\n",
    "query = {\"input\": \"Who is Godzilla\"}\n",
    "\n",
    "# Run the chain using the query as input and print the result.\n",
    "result = agent.run(query)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\gradio\\utils.py:915: UserWarning: Expected 3 arguments for function <function pdfBot at 0x0000016EF4252020>, received 2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\gradio\\utils.py:919: UserWarning: Expected at least 3 arguments for function <function pdfBot at 0x0000016EF4252020>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Gradio Blocks instance for structuring the interface\n",
    "app = gr.Blocks(fill_height=True)\n",
    "\n",
    "#ChatBot function\n",
    "def respond(message, chat_history):\n",
    "    # Set up the built-in wikipedia tool.\n",
    "    tools = load_tools(['wikipedia'], llm=llm)\n",
    "\n",
    "    # Initialize the agent.\n",
    "    agent = initialize_agent(tools, agent=\"chat-zero-shot-react-description\", handle_parsing_errors=True ,llm=llm)\n",
    "    bot_message = agent.run(message)\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "\n",
    "#Forecast bot function\n",
    "def pdfBot(query, file, file_data):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(stream=file_data, filetype=\"pdf\")\n",
    "\n",
    "        # Iterate over each page in the PDF\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    format_template = f\"{text}\\n\\n{query}\"\n",
    "\n",
    "    print(format_template)\n",
    "\n",
    "    input_variables = [\"query\"]\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=format_template\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    result = chain.invoke({\"query\": query})\n",
    "\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "# Add tabs to the Gradio Blocks\n",
    "with app:\n",
    "    #A chatbot that will be used to talk with our first model we trained\n",
    "    with gr.Tab(\"ChatBot\"):\n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(placeholder=\"Ask Your Question\", show_label=False, scale=1)\n",
    "        clear = gr.ClearButton([msg, chatbot])\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "\n",
    "    with gr.Tab(\"PDFUploader\"):\n",
    "        gr.File(label=\"Upload a file:\", type='binary'),\n",
    "        \n",
    "    #A basic input/output which will take some text and output a prediction that will be used with our second Model\n",
    "    with gr.Tab(\"PDF Bot\"):\n",
    "        with gr.Row():\n",
    "\n",
    "        \n",
    "            gr.Interface(fn=pdfBot, allow_flagging='never',\n",
    "                            inputs=[\n",
    "                            gr.Textbox(lines=1, placeholder=\"Input\", label=\"Question\"),\n",
    "                            gr.File(label=\"Upload a file:\", type='binary'),\n",
    "                            #gr.CheckboxGroup([\"USA\", \"Japan\", \"Canada\"], label=\"Countries\", info=\"Where is your prediction\"),\n",
    "                            ],\n",
    "                            outputs=gr.Textbox(lines=12,label=\"Results\", show_copy_button=True, interactive=False))\n",
    "\n",
    "\n",
    "# Launch the app\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
