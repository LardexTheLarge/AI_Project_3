{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz  \n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Store the API key in a variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"./Models/emission.h5\")\n",
    "\n",
    "# Load the model to a new object\n",
    "model = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\gradio\\interface.py:377: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Gradio Blocks instance for structuring the interface\n",
    "app = gr.Blocks(fill_height=True)\n",
    "\n",
    "#ChatBot function\n",
    "def chatbotfunc(message, chat_history):\n",
    "    # Set up the built-in Wikipedia tool.\n",
    "    tools = load_tools(['wikipedia'], llm=llm)\n",
    "\n",
    "    # Initialize the agent with specified parameters.\n",
    "    # 'agent' parameter specifies the type of agent to use ('chat-zero-shot-react-description').\n",
    "    # 'handle_parsing_errors' parameter is set to True to handle any parsing errors that may occur.\n",
    "    # 'llm' parameter is passed to the agent for language model configuration.\n",
    "    agent = initialize_agent(tools, agent=\"chat-zero-shot-react-description\", handle_parsing_errors=True, llm=llm)\n",
    "\n",
    "    # Run the agent to generate a response based on the input message.\n",
    "    bot_message = agent.run(message)\n",
    "\n",
    "    # Append the message and bot's response to the chat history.\n",
    "    chat_history.append((message, bot_message))\n",
    "\n",
    "    # Return an empty string (response) along with the updated chat history.\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "\n",
    "# # Define a function to make predictions\n",
    "# def make_predictions(year, emission, capTrade, Etarget, energy):\n",
    "#     try:\n",
    "#         # Apply value mapping for capTrade\n",
    "#         if capTrade == \"Yes\":\n",
    "#             capTrade_value = 1\n",
    "#         elif capTrade == \"No\":\n",
    "#             capTrade_value = 0\n",
    "#         else:\n",
    "#             raise ValueError(\"capTrade must be either 'Yes' or 'No'\")\n",
    "\n",
    "#         # Create a dataframe with 5 columns containing the input number\n",
    "#         df = pd.DataFrame([year, emission, capTrade_value, Etarget, energy])\n",
    "        \n",
    "#         # Convert the dataframe to a numpy array and reshape it to match the model input shape\n",
    "#         input_array = df.values.reshape(1, -1)  # Reshape based on dataframe shape\n",
    "        \n",
    "#         # Generate predictions using the loaded model\n",
    "#         predictions = model.predict(input_array, verbose=0)\n",
    "        \n",
    "#         return f\"{predictions[0][0]:.2f}°F\"\n",
    "\n",
    "#     except ValueError as ve:\n",
    "#         return f\"ValueError: {ve}\"\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {e}\"\n",
    "    \n",
    "\n",
    "# Define a function to make predictions\n",
    "def make_predictions(year, emission, capTrade, eTarget, energy):\n",
    "    try:\n",
    "        # Apply value mapping for capTrade\n",
    "        if capTrade == \"Yes\":\n",
    "            capTrade_value = 1\n",
    "        elif capTrade == \"No\":\n",
    "            capTrade_value = 0\n",
    "        else:\n",
    "            raise ValueError(\"capTrade must be either 'Yes' or 'No'\")\n",
    "\n",
    "        # Create a dataframe with 5 columns containing the input number\n",
    "        df = pd.DataFrame([year, emission, capTrade_value, eTarget, energy])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "        df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "        \n",
    "        # Convert the dataframe to a numpy array and reshape it to match the model input shape\n",
    "        input_array = df_scaled.values.reshape(1, -1)  # Reshape based on dataframe shape\n",
    "        \n",
    "        # Generate predictions using the loaded model\n",
    "        predictions = model.predict(input_array, verbose=0)\n",
    "        \n",
    "        return f\"{predictions[0][0]:.2f}°F\"\n",
    "\n",
    "    except ValueError as ve:\n",
    "        return f\"ValueError: {ve}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "    # Function to preprocess the file\n",
    "def predict_data(input_file):\n",
    "    # Load the data from the input file\n",
    "    df = pd.read_csv(input_file)\n",
    "    #Drop country from data frame\n",
    "    df.drop(columns='Country', axis=1,inplace=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "\n",
    "    # Make predictions using the loaded Keras model\n",
    "    predictions = model.predict(df_scaled)\n",
    "\n",
    "    #append predicted values\n",
    "    df['temp_change'] = predictions\n",
    "\n",
    "    mean_temp_change =df.groupby('Year')['temp_change'].mean()\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(mean_temp_change.index, mean_temp_change.values, marker='o')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Mean Temperature Change')\n",
    "    plt.title('Mean Temperature Change by Year')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(mean_temp_change.index)\n",
    "    \n",
    "    plt.savefig('output_chart.png')\n",
    "\n",
    "    return 'output_chart.png'\n",
    "\n",
    "\n",
    "\n",
    "def pdfBot(query, file_data):\n",
    "    # Check if file_data is None (no file uploaded)\n",
    "    if file_data is None:\n",
    "        return \"Error: No PDF file uploaded.\"\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(stream=file_data, filetype=\"pdf\")\n",
    "\n",
    "        # Iterate over each page in the PDF\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "    # Check if no text was extracted\n",
    "    if not text:\n",
    "        return \"Error: No text extracted from the PDF.\"\n",
    "\n",
    "    # Construct the format template\n",
    "    format_template = f\"{text}\\n\\n{query}\"\n",
    "\n",
    "    # Define input variables for the prompt template\n",
    "    input_variables = [\"query\"]\n",
    "\n",
    "    # Create a prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=format_template\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain instance\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain with the query\n",
    "    result = chain.invoke({\"query\": query})\n",
    "\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "# Define the interface using Gradio.\n",
    "with app:\n",
    "    # Create a tab for the ChatBot.\n",
    "    with gr.Tab(\"ChatBot\"):\n",
    "        # Initialize a Chatbot component.\n",
    "        chatbot = gr.Chatbot()\n",
    "\n",
    "        # Create a textbox for user input without a label and scale it to size 1.\n",
    "        msg = gr.Textbox(placeholder=\"Ask Your Question\", show_label=False, scale=1, value=\"What is Cap and Trade?\")\n",
    "\n",
    "        # Create a ClearButton component that clears the textbox and chatbot when clicked.\n",
    "        clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "        # Submit user input from the textbox to the chatbot when submitted.\n",
    "        msg.submit(chatbotfunc, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "\n",
    "    with gr.Tab(\"PredictionBot\"):\n",
    "        years = list(range(2025, 2040))\n",
    "        gr.Interface(\n",
    "            fn=make_predictions,  # Use the prediction function as the function to be executed\n",
    "            allow_flagging='never',\n",
    "            inputs = [\n",
    "                    gr.Dropdown(choices=years, label=\"Select Year\", value=2026),  # Input component for specifying number of predictions\n",
    "                    gr.Number(label=\"Current Emissions (Metric Ton)\", value=36729217),  # Input component for specifying number of predictions\n",
    "                    gr.Radio(choices=[\"Yes\", \"No\"], label=\"Choose an option\", value=\"Yes\"), # Input component for specifying number of predictions\n",
    "                    gr.Number(label=\"Emission Target\", value=472),  # Input component for specifying number of predictions\n",
    "                    gr.Number(label=\"Current Renewable Energy\", value=1519),  # Input component for specifying number of predictions\n",
    "            ],\n",
    "            outputs=gr.Textbox(label=\"Temperature Prediction\", show_copy_button=True, interactive=False),  # Output component to display model predictions\n",
    "            title=\"Model Predictions\",  # Title for the interface\n",
    "            description=\"Enter the values you need in their corresponding inputs.\",  # Description for the interface\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"VisualBot\"):\n",
    "        gr.Interface(fn=predict_data, \n",
    "                     inputs=gr.File(label=\"Upload Data File\"), \n",
    "                     outputs=\"image\",\n",
    "                     allow_flagging=False)\n",
    "\n",
    "    # Create a tab for the PDF Bot.\n",
    "    with gr.Tab(\"PDFBot\"):\n",
    "            # Define the Gradio interface for the PDF Bot.\n",
    "            gr.Interface(fn=pdfBot, title=\"Upload Your Desired PDF\", allow_flagging='never',\n",
    "                        inputs=[\n",
    "                            gr.Textbox(lines=1, placeholder=\"Input\", label=\"Ask a Question about your PDF:\", value=\"What is this PDF about?\"),\n",
    "                            gr.File(label=\"12 pages or less\", type='binary'),\n",
    "                        ],\n",
    "                        outputs=gr.Textbox(lines=16, label=\"Answer:\", show_copy_button=True, interactive=False))\n",
    "\n",
    "\n",
    "\n",
    "# Launch the app\n",
    "app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
